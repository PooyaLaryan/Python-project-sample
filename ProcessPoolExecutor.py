import time
import concurrent.futures
import torch  # ÙÙ‚Ø· Ø§Ú¯Ø± Ú©Ø§Ø± GPU Ø¯Ø§Ø±ÛŒ

def heavy_task_one():
    print("ğŸ”¹ Task 1 (GPU/CPU) started...")
    time.sleep(1)  # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡

    # Ù…Ø«Ø§Ù„: Ø¹Ù…Ù„ÛŒØ§Øª Ù…Ø§ØªØ±ÛŒØ³ÛŒ Ø±ÙˆÛŒ GPU (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Task 1 running on: {device}")

    x = torch.randn(5000, 5000, device=device)
    y = torch.randn(5000, 5000, device=device)
    z = torch.matmul(x, y)  # Ø¶Ø±Ø¨ Ù…Ø§ØªØ±ÛŒØ³ÛŒ Ø³Ù†Ú¯ÛŒÙ†
    torch.cuda.synchronize() if device == "cuda" else None

    print("âœ… Task 1 finished.")
    return f"Result 1 (sum={z.sum().item():.4f})"

def heavy_task_two():
    print("ğŸ”¹ Task 2 (GPU/CPU) started...")
    time.sleep(1)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Task 2 running on: {device}")

    x = torch.randn(4000, 4000, device=device)
    y = torch.randn(4000, 4000, device=device)
    z = torch.matmul(x, y)
    torch.cuda.synchronize() if device == "cuda" else None

    print("âœ… Task 2 finished.")
    return f"Result 2 (mean={z.mean().item():.4f})"

def main():
    print("ğŸš€ Starting heavy GPU/CPU tasks in parallel...")

    # Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ø³ØªÙ‚Ù„
    with concurrent.futures.ProcessPoolExecutor() as executor:
        future1 = executor.submit(heavy_task_one)
        future2 = executor.submit(heavy_task_two)

        # Ù…Ù†ØªØ¸Ø± Ù…ÛŒâ€ŒÙ…Ø§Ù†ÛŒÙ… ØªØ§ Ù‡Ø± Ø¯Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªÙ…Ø§Ù… Ø´ÙˆÙ†Ø¯
        result1 = future1.result()
        result2 = future2.result()

    print("\nğŸ¯ Both tasks completed!")
    print(result1)
    print(result2)
    print("â¡ Continuing the rest of the program...")

if __name__ == "__main__":
    main()
