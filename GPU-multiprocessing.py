import torch
import torch.multiprocessing as mp
import os
import time

def heavy_gpu_task(rank, return_dict):
    """
    rank: Ø´Ù…Ø§Ø±Ù‡â€ŒÛŒ GPU Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø±ÙˆÛŒ Ø¢Ù† Ø§Ø¬Ø±Ø§ Ø´ÙˆØ¯
    return_dict: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø§Ø´ØªØ±Ø§Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù†ØªÛŒØ¬Ù‡ Ù‡Ø± GPU
    """
    device = f"cuda:{rank}" if torch.cuda.is_available() else "cpu"
    print(f"[GPU {rank}] Task started on {device}")

    torch.cuda.set_device(rank)
    start_time = time.time()

    # Ú©Ø§Ø± Ø³Ù†Ú¯ÛŒÙ† â€” Ø¶Ø±Ø¨ Ø¯Ùˆ Ù…Ø§ØªØ±ÛŒØ³ Ø¨Ø²Ø±Ú¯
    x = torch.randn(6000, 6000, device=device)
    y = torch.randn(6000, 6000, device=device)
    z = torch.matmul(x, y)
    torch.cuda.synchronize()

    duration = time.time() - start_time
    result_value = z.mean().item()

    print(f"[GPU {rank}] âœ… Task finished in {duration:.2f} sec")
    return_dict[rank] = (result_value, duration)

def main():
    if not torch.cuda.is_available():
        print("âŒ No GPU found. Please run on a system with CUDA support.")
        return

    gpu_count = torch.cuda.device_count()
    print(f"ğŸš€ Found {gpu_count} GPU(s). Launching parallel processes...")

    # Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø§Ø´ØªØ±Ø§Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§
    manager = mp.Manager()
    return_dict = manager.dict()

    # Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© Process Ø¨Ø±Ø§ÛŒ Ù‡Ø± GPU
    processes = []
    for rank in range(gpu_count):
        p = mp.Process(target=heavy_gpu_task, args=(rank, return_dict))
        p.start()
        processes.append(p)

    # Ù…Ù†ØªØ¸Ø± Ù…Ø§Ù†Ø¯Ù† Ø¨Ø±Ø§ÛŒ Ø§ØªÙ…Ø§Ù… Ù‡Ù…Ù‡ ÙØ±Ø¢ÛŒÙ†Ø¯Ù‡Ø§
    for p in processes:
        p.join()

    print("\nğŸ¯ All GPU tasks completed.")
    for gpu_id, (result, duration) in return_dict.items():
        print(f"  ğŸŸ© GPU {gpu_id}: mean={result:.5f}, time={duration:.2f} sec")

    print("\nâ¡ Continuing the rest of the program...")

if __name__ == "__main__":
    os.environ["OMP_NUM_THREADS"] = "1"  # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² oversubscription
    mp.set_start_method("spawn", force=True)
    main()
